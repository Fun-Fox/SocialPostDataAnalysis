import whisperx  # WhisperX æ˜¯ä¸€ä¸ªæ”¯æŒè¯­éŸ³è¯†åˆ«å’Œè¯´è¯äººåˆ†å‰²çš„åº“
import os
import torch
import gc
from typing import List, Dict, Union


# é…ç½®è·¯å¾„
def extract_audio(video_path: str, audio_dir: str = "audio") -> str | None:
    """
    ä»è§†é¢‘æ–‡ä»¶ä¸­æå–éŸ³é¢‘å¹¶ä¿å­˜ä¸º MP3 æ ¼å¼
    :param video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
    :param audio_dir: æå–åçš„éŸ³é¢‘ä¿å­˜ç›®å½•
    :return: è¿”å›éŸ³é¢‘æ–‡ä»¶è·¯å¾„æˆ– Noneï¼ˆå¤±è´¥æ—¶ï¼‰
    """
    os.makedirs(audio_dir, exist_ok=True)
    video_name = os.path.splitext(os.path.basename(video_path))[0]  # è·å–ä¸å¸¦æ‰©å±•åçš„è§†é¢‘æ–‡ä»¶å
    audio_file = os.path.join(audio_dir, f"{video_name}.mp3")  # æ„å»ºè¾“å‡ºéŸ³é¢‘æ–‡ä»¶è·¯å¾„

    if os.path.exists(audio_file):
        return audio_file  # å¦‚æœå·²ç»å­˜åœ¨è¯¥éŸ³é¢‘ï¼Œåˆ™ç›´æ¥è¿”å›è·¯å¾„

    try:
        from moviepy import VideoFileClip
        clip = VideoFileClip(video_path)  # åŠ è½½è§†é¢‘æ–‡ä»¶
        if clip.audio is None:
            raise ValueError("è¯¥è§†é¢‘æ–‡ä»¶æ²¡æœ‰éŸ³é¢‘è½¨é“")
        clip.audio.write_audiofile(audio_file)  # å†™å…¥éŸ³é¢‘æ–‡ä»¶
        print(f"éŸ³é¢‘æå–å®Œæˆ: {audio_file}")
        return audio_file
    except Exception as e:
        print(f"âŒ æå–éŸ³é¢‘å¤±è´¥ ({video_path}): {str(e)}")
        return None


def batch_extract_audio_from_directory(video_dir: str, audio_dir: str = "audio"):
    """
    æ‰¹é‡æå–æŒ‡å®šç›®å½•ä¸‹æ‰€æœ‰è§†é¢‘çš„éŸ³é¢‘
    :param video_dir: è§†é¢‘æ–‡ä»¶å¤¹è·¯å¾„
    :param audio_dir: éŸ³é¢‘ä¿å­˜è·¯å¾„
    :return: æå–æˆåŠŸçš„éŸ³é¢‘æ–‡ä»¶è·¯å¾„åˆ—è¡¨
    """
    supported_exts = ['.mp4', '.mkv', '.avi', '.mov']  # æ”¯æŒçš„è§†é¢‘æ ¼å¼
    video_files = [
        os.path.join(video_dir, f)
        for f in os.listdir(video_dir)
        if os.path.splitext(f)[1].lower() in supported_exts
    ]

    for video_file in video_files:
        print(f"ğŸµ æ­£åœ¨æå– {video_file} çš„éŸ³é¢‘...")
        extract_audio(video_file, audio_dir)

    print(f"âœ… å·²æå–å…¨éƒ¨éŸ³é¢‘åˆ° {audio_dir}")

    return [os.path.join(audio_dir, f"{os.path.splitext(os.path.basename(vf))[0]}.mp3") for vf in video_files]


# ä¸»å¤„ç†å‡½æ•°
def process_audio_files(audio_paths: Union[str, List[str]], hf_token: str, device="cuda", compute_type="float16",
                        batch_size=16):
    """
    å¤„ç†å¤šä¸ªéŸ³é¢‘æ–‡ä»¶ï¼šè¯­éŸ³è¯†åˆ« + æ—¶é—´å¯¹é½ + è¯´è¯äººåˆ†é…
    :param audio_paths: å•ä¸ªæˆ–å¤šä¸ªéŸ³é¢‘æ–‡ä»¶è·¯å¾„
    :param hf_token: HuggingFace è®¤è¯ Token
    :param device: ä½¿ç”¨è®¾å¤‡ ('cuda' æˆ– 'cpu')
    :param compute_type: æ¨¡å‹è®¡ç®—ç±»å‹ (float16 / int8)# change to "int8" if low on GPU mem (may reduce accuracy)
    :param batch_size: æ‰¹å¤„ç†å¤§å°
    :return: åŒ…å«è¯†åˆ«ç»“æœçš„åˆ—è¡¨
    """
    results = []

    if isinstance(audio_paths, str):
        audio_paths = [audio_paths]

    for audio_file in audio_paths:
        print(f"ğŸ”„ å¤„ç†éŸ³é¢‘æ–‡ä»¶: {audio_file}")
        try:
            # ç¬¬ä¸€æ­¥ï¼šåŠ è½½ WhisperX æ¨¡å‹å¹¶è½¬å½•éŸ³é¢‘
            # model = whisperx.load_model("large-v2", device, compute_type=compute_type)
            # save model to local path (optional)
            model_dir = "/path/"
            model = whisperx.load_model("large-v2", device, compute_type=compute_type, download_root=model_dir)

            audio = whisperx.load_audio(audio_file)
            result = model.transcribe(audio, batch_size=batch_size)
            print(result["segments"])  # è¾“å‡ºåˆæ­¥è¯†åˆ«ç»“æœ

            # æ¸…ç†æ¨¡å‹ä»¥é‡Šæ”¾ GPU æ˜¾å­˜ï¼ˆå¯é€‰ï¼‰
            # del model
            # gc.collect()
            # torch.cuda.empty_cache()

            # ç¬¬äºŒæ­¥ï¼šå¯¹è¯†åˆ«ç»“æœè¿›è¡Œæ—¶é—´æˆ³å¯¹é½
            model_a, metadata = whisperx.load_align_model(language_code=result["language"], device=device)
            result = whisperx.align(result["segments"], model_a, metadata, audio, device)

            # æ¸…ç†å¯¹é½æ¨¡å‹èµ„æºï¼ˆå¯é€‰ï¼‰
            # del model_a
            # gc.collect()
            # torch.cuda.empty_cache()

            # ç¬¬ä¸‰æ­¥ï¼šä½¿ç”¨ Diarization æ¨¡å‹åˆ†é…è¯´è¯äººæ ‡ç­¾
            diarize_model = whisperx.diarize.DiarizationPipeline(use_auth_token=hf_token, device=device)
            diarize_segments = diarize_model(audio)
            result = whisperx.assign_word_speakers(diarize_segments, result)

            # è¾“å‡ºæœ€ç»ˆåŒ…å«è¯´è¯äººæ ‡ç­¾çš„ç»“æœ
            print(result["segments"])  # segments ç°åœ¨åŒ…å« speaker ID

            # å­˜å‚¨å½“å‰éŸ³é¢‘çš„è¯†åˆ«ç»“æœ
            results.append({
                "audio_file": audio_file,
                "result": result["segments"]
            })

        except Exception as e:
            # æ•è·å¼‚å¸¸å¹¶è®°å½•é”™è¯¯ä¿¡æ¯
            results.append({
                "audio_file": audio_file,
                "error": str(e)
            })

    return results
